{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ehtRWB8zUo"
      },
      "outputs": [],
      "source": [
        "#Step -1\n",
        "#importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Set to display all the columns of the dataset\n",
        "pd.set_option(\"display.max_columns\",None)\n",
        "\n",
        "#Ignore harmless warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define constants\n",
        "data_path = r\"/home/l2-37/Downloads/1325_images-20241016T064413Z-001/1325_images\"\n",
        "target_size = (224, 224)  # Define the target size for resizing\n",
        "\n",
        "# Load and preprocess data\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each class (Monkeypox and Non_Monkeypox)\n",
        "for class_name in ['Non_Monkeypox', 'Monkeypox']:\n",
        "    class_path = os.path.join(data_path, class_name)\n",
        "    # Iterate through each image in the class\n",
        "    for image_name in os.listdir(class_path):\n",
        "        image_path = os.path.join(class_path, image_name)\n",
        "        # Read the image in grayscale\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        # Resize the image to the target size\n",
        "        img = cv2.resize(img, target_size)\n",
        "        # Normalize the pixel values to the range [0, 1]\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        # Append the preprocessed image to the images list\n",
        "        images.append(img)\n",
        "        # Append the label (0 for Non_Monkeypox, 1 for Monkeypox) to the labels list\n",
        "        labels.append(0 if class_name == 'Non_Monkeypox' else 1)\n",
        "\n",
        "# Convert images and labels to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print class proportions\n",
        "print(\"Class Proportions:\")\n",
        "print(\"Non_Monkeypox:\", np.sum(labels == 0), \"Monkeypox:\", np.sum(labels == 1))\n",
        "\n",
        "# Plot histogram for class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(labels, bins=2, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.xticks([0.25, 0.75], ['Non_Monkeypox', 'Monkeypox'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Data Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pPvmS_9O849E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance the dataset with 600 images from each class (Non_Monkeypox and Monkeypox)\n",
        "non_monkeypox_images = images[labels == 0][:600]\n",
        "monkeypox_images = images[labels == 1][:600]\n",
        "non_monkeypox_labels = labels[labels == 0][:600]\n",
        "monkeypox_labels = labels[labels == 1][:600]\n",
        "\n",
        "# Concatenate the data back together\n",
        "balanced_images = np.concatenate([non_monkeypox_images, monkeypox_images])\n",
        "balanced_labels = np.concatenate([non_monkeypox_labels, monkeypox_labels])\n",
        "\n",
        "# Split the balanced dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(balanced_images, balanced_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the balanced data\n",
        "print(\"Balanced Images:\", balanced_images.shape)\n",
        "print(\"Balanced Labels:\", balanced_labels.shape)\n",
        "\n",
        "# Print the data separately for each class\n",
        "print(\"Non_Monkeypox Images:\", non_monkeypox_images.shape)\n",
        "print(\"Monkeypox Images:\", monkeypox_images.shape)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "VmtRDOEJ86n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram for class distribution before splitting\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Before splitting (Original data)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(labels, bins=2, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.xticks([0.25, 0.75], ['Non_Monkeypox', 'Monkeypox'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Data Distribution Before Splitting')\n",
        "\n",
        "# After splitting (Train and Test sets)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_train, bins=2, color='lightcoral', edgecolor='black', alpha=0.7, label='Train')\n",
        "plt.hist(y_test, bins=2, color='mediumseagreen', edgecolor='black', alpha=0.7, label='Test')\n",
        "plt.xticks([0.25, 0.75], ['Non_Monkeypox', 'Monkeypox'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Data Distribution After Splitting')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7f8GjcFh86qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def check_dataset_balance(labels):\n",
        "    # Count the occurrences of each class label\n",
        "    label_counts = Counter(labels)\n",
        "\n",
        "    # Print the counts for each class\n",
        "    print(\"Class distribution in the dataset:\")\n",
        "    for label, count in label_counts.items():\n",
        "        class_name = \"Monkeypox\" if label == 1 else \"Non_Monkeypox\"\n",
        "        print(f\"{class_name}: {count} images\")\n",
        "\n",
        "    # Check if the dataset is balanced\n",
        "    counts = list(label_counts.values())\n",
        "    min_count = min(counts)\n",
        "    max_count = max(counts)\n",
        "\n",
        "    is_balanced = min_count == max_count\n",
        "\n",
        "    if is_balanced:\n",
        "        print(\"The dataset is balanced.\")\n",
        "    else:\n",
        "        print(\"The dataset is not balanced.\")\n",
        "        print(f\"Minimum count: {min_count}, Maximum count: {max_count}\")\n",
        "\n",
        "# Call the function with balanced_labels\n",
        "check_dataset_balance(balanced_labels)\n"
      ],
      "metadata": {
        "id": "LGHvNIaP86tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the number of images to display\n",
        "num_images_to_display = 10\n",
        "\n",
        "# Select random indices for the images\n",
        "indices = random.sample(range(len(balanced_images)), num_images_to_display)\n",
        "\n",
        "# Create a figure to display the images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loop over the selected indices and display the images\n",
        "for i, idx in enumerate(indices):\n",
        "    # Get the image and its corresponding label\n",
        "    image = balanced_images[idx]\n",
        "    label = balanced_labels[idx]\n",
        "\n",
        "    # Add a subplot for the current image\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f'Label: {\"Monkeypox\" if label == 1 else \"Non_Monkeypox\"}')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Adjust the layout of the subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IPdu8Dmg86wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the number of images to display\n",
        "num_images_to_display = 10\n",
        "\n",
        "# Select random indices for the images\n",
        "indices = random.sample(range(len(balanced_images)), num_images_to_display)\n",
        "\n",
        "# Create a figure to display the images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loop over the selected indices and display the images\n",
        "for i, idx in enumerate(indices):\n",
        "    # Get the image and its corresponding label\n",
        "    image = balanced_images[idx]\n",
        "    label = balanced_labels[idx]\n",
        "\n",
        "    # Add a subplot for the current image\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Adjust the layout of the subplots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d12ceyyk860y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "-CJ0Ujld863k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseLayers"
      ],
      "metadata": {
        "id": "uRNY-NPd9OQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "gVPsBLdb-BEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top layers\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for binary classification\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives(),\n",
        "                                                                      tf.keras.metrics.TruePositives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tn, fp, fn, tp = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "spEA-DhO9H1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "KQ8lD-w0-DT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top layers\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for binary classification\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives(),\n",
        "                                                                      tf.keras.metrics.TruePositives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tn, fp, fn, tp = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "AT7hGq72-EsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "SaxfxVRL-L3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top layers\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for binary classification\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives(),\n",
        "                                                                      tf.keras.metrics.TruePositives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tn, fp, fn, tp = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "dctOkjiY-Q42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GlobalAveragePooling2D"
      ],
      "metadata": {
        "id": "3Hqf_MnN-Ty3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, accuracy_score  # Add accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      ])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_resized) > 0.5\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall (Sensitivity): {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'Area Under the ROC Curve (AUC-ROC): {roc_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall curve and AUC score\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'Precision-Recall Curve (AUC-PR): {pr_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "Bj2QLll6-XN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "CHHfuaXR-i-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, accuracy_score  # Add accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      ])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_resized) > 0.5\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall (Sensitivity): {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'Area Under the ROC Curve (AUC-ROC): {roc_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall curve and AUC score\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'Precision-Recall Curve (AUC-PR): {pr_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "PLTLKueJ-kXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "ed7czUzJ-oKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score, accuracy_score  # Add accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      ])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_resized) > 0.5\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall (Sensitivity): {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Specificity\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, test_predictions).ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'Area Under the ROC Curve (AUC-ROC): {roc_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall curve and AUC score\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'Precision-Recall Curve (AUC-PR): {pr_auc:.2f}')\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "pF99amZF-p3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv2D + GlobalAveragePooling2D +Dropout+ Dense Layers"
      ],
      "metadata": {
        "id": "-Y5Hx-gy-ti5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "YgjenY6V_JYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "GVrZ4iIO-suc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "tTldZ7n2_Ls2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "pd2iP2ml_M3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "nBfe_iuv_PKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Define custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "2S-OcLeE_QQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM + Dense Layers"
      ],
      "metadata": {
        "id": "ayIYZJ5i_Um0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "_Zry6F3G_VqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match NASNetMobile input\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top (no classification layers)\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional layers of NASNetMobile\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Pass the input through NASNetMobile model\n",
        "nasnet_output = base_model(input_layer, training=False)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(nasnet_output)\n",
        "\n",
        "# Reshape for LSTM\n",
        "reshape = Reshape((1, -1))(flatten)\n",
        "\n",
        "# Define the LSTM layer\n",
        "lstm = LSTM(128)(reshape)\n",
        "\n",
        "# Define the dense layers for classification\n",
        "dense1 = Dense(64, activation='relu')(lstm)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "FAev1VAx_VNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "rS8aY7WC_baM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match NASNetMobile input\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top (no classification layers)\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional layers of NASNetMobile\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Pass the input through NASNetMobile model\n",
        "nasnet_output = base_model(input_layer, training=False)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(nasnet_output)\n",
        "\n",
        "# Reshape for LSTM\n",
        "reshape = Reshape((1, -1))(flatten)\n",
        "\n",
        "# Define the LSTM layer\n",
        "lstm = LSTM(128)(reshape)\n",
        "\n",
        "# Define the dense layers for classification\n",
        "dense1 = Dense(64, activation='relu')(lstm)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "t2YlsJ7e_cqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "b3QGsogb_eSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension to match NASNetMobile input\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load NASNetMobile model without top (no classification layers)\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional layers of NASNetMobile\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Pass the input through NASNetMobile model\n",
        "nasnet_output = base_model(input_layer, training=False)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(nasnet_output)\n",
        "\n",
        "# Reshape for LSTM\n",
        "reshape = Reshape((1, -1))(flatten)\n",
        "\n",
        "# Define the LSTM layer\n",
        "lstm = LSTM(128)(reshape)\n",
        "\n",
        "# Define the dense layers for classification\n",
        "dense1 = Dense(64, activation='relu')(lstm)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "0KXUpn3G_hVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU + Dense Layers"
      ],
      "metadata": {
        "id": "lC33AJgE_0ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "-njmYDxE_0_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GRU, Dense, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Clear previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),  # Convolutional layer with 32 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer with 64 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer with 128 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(256, (3, 3), activation='relu'),  # Convolutional layer with 256 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Flatten(),  # Flatten layer to flatten the output of the convolutional layers\n",
        "    Reshape((1, -1)),  # Reshape to 3D tensor for GRU\n",
        "    GRU(128),  # GRU layer with 128 units\n",
        "    Dense(1, activation='sigmoid')  # Dense layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "VHqbADCJ_3Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "7gToGT3IAA7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GRU, Dense, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Clear previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),  # Convolutional layer with 32 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer with 64 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer with 128 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(256, (3, 3), activation='relu'),  # Convolutional layer with 256 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Flatten(),  # Flatten layer to flatten the output of the convolutional layers\n",
        "    Reshape((1, -1)),  # Reshape to 3D tensor for GRU\n",
        "    GRU(128),  # GRU layer with 128 units\n",
        "    Dense(1, activation='sigmoid')  # Dense layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "22hf5V-LACBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "pYyn9e77AE6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GRU, Dense, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import cv2\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Clear previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),  # Convolutional layer with 32 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer with 64 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer with 128 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Conv2D(256, (3, 3), activation='relu'),  # Convolutional layer with 256 filters and 3x3 kernel\n",
        "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
        "    Flatten(),  # Flatten layer to flatten the output of the convolutional layers\n",
        "    Reshape((1, -1)),  # Reshape to 3D tensor for GRU\n",
        "    GRU(128),  # GRU layer with 128 units\n",
        "    Dense(1, activation='sigmoid')  # Dense layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR'),\n",
        "                                                                      tf.keras.metrics.TruePositives(),\n",
        "                                                                      tf.keras.metrics.FalsePositives(),\n",
        "                                                                      tf.keras.metrics.TrueNegatives(),\n",
        "                                                                      tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr, tp, fp, tn, fn = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "print(f'TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "PyLtUlbuAGIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capsule Network"
      ],
      "metadata": {
        "id": "s5hTkGFjANzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "ApFHaG1NAPIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, Reshape, Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "\n",
        "# Define Capsule Layer\n",
        "class CapsuleLayer(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        self.W = self.add_weight(shape=[input_dim_capsule, self.num_capsule * self.dim_capsule],\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 name='W')\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        u_hat_vecs = tf.reduce_sum(tf.expand_dims(u_vecs, -1) * tf.expand_dims(self.W, 0), axis=-2)\n",
        "        b = tf.zeros(shape=[tf.shape(u_hat_vecs)[0], self.num_capsule])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            s_j = tf.reduce_sum(tf.multiply(c[:, :, None], u_hat_vecs), axis=1)\n",
        "            v_j = self.squash(s_j)\n",
        "            if i < self.routings - 1:\n",
        "                b += tf.reduce_sum(tf.multiply(v_j[:, None, :], u_hat_vecs), axis=-1)\n",
        "        return v_j\n",
        "\n",
        "    def squash(self, s_j):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(s_j), axis=-1, keepdims=True)\n",
        "        scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale * s_j\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "X_train_resized = np.repeat(X_train_resized, 3, axis=-1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "X_test_resized = np.repeat(X_test_resized, 3, axis=-1)\n",
        "\n",
        "# Define your custom CNN architecture with Capsule Network\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Reshape((-1, 128))(x)  # Reshape to 3D tensor for Capsule Network\n",
        "capsule = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(x)  # Example: 10 capsules with dimension 16\n",
        "capsule = Flatten()(capsule)  # Flatten the output\n",
        "output = Dense(1, activation='sigmoid')(capsule)  # Use a single output unit for binary classification\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=input_tensor, outputs=output)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score, Cohen's Kappa, Matthews Correlation Coefficient, and Balanced Accuracy\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Cohen's Kappa Coefficient:\", kappa)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)\n",
        "\n",
        "# Load NASNetMobile as base model\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define custom CNN architecture with attention mechanism on top of NASNetMobile\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "attention_probs = Dense(np.prod(x.shape[1:]), activation='softmax', name='attention_probs')(x)\n",
        "attention_mul = Multiply()([x, attention_probs])\n",
        "output_layer = Dense(1, activation='sigmoid')(attention_mul)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "t8R2Ng0dAOre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "1mxjccjwAUPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, Reshape, Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "\n",
        "# Define Capsule Layer\n",
        "class CapsuleLayer(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        self.W = self.add_weight(shape=[input_dim_capsule, self.num_capsule * self.dim_capsule],\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 name='W')\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        u_hat_vecs = tf.reduce_sum(tf.expand_dims(u_vecs, -1) * tf.expand_dims(self.W, 0), axis=-2)\n",
        "        b = tf.zeros(shape=[tf.shape(u_hat_vecs)[0], self.num_capsule])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            s_j = tf.reduce_sum(tf.multiply(c[:, :, None], u_hat_vecs), axis=1)\n",
        "            v_j = self.squash(s_j)\n",
        "            if i < self.routings - 1:\n",
        "                b += tf.reduce_sum(tf.multiply(v_j[:, None, :], u_hat_vecs), axis=-1)\n",
        "        return v_j\n",
        "\n",
        "    def squash(self, s_j):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(s_j), axis=-1, keepdims=True)\n",
        "        scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale * s_j\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "X_train_resized = np.repeat(X_train_resized, 3, axis=-1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "X_test_resized = np.repeat(X_test_resized, 3, axis=-1)\n",
        "\n",
        "# Define your custom CNN architecture with Capsule Network\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Reshape((-1, 128))(x)  # Reshape to 3D tensor for Capsule Network\n",
        "capsule = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(x)  # Example: 10 capsules with dimension 16\n",
        "capsule = Flatten()(capsule)  # Flatten the output\n",
        "output = Dense(1, activation='sigmoid')(capsule)  # Use a single output unit for binary classification\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=input_tensor, outputs=output)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score, Cohen's Kappa, Matthews Correlation Coefficient, and Balanced Accuracy\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Cohen's Kappa Coefficient:\", kappa)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)\n",
        "\n",
        "# Load NASNetMobile as base model\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define custom CNN architecture with attention mechanism on top of NASNetMobile\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "attention_probs = Dense(np.prod(x.shape[1:]), activation='softmax', name='attention_probs')(x)\n",
        "attention_mul = Multiply()([x, attention_probs])\n",
        "output_layer = Dense(1, activation='sigmoid')(attention_mul)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "NH3uSPbLAVZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "2DxPbpM0AX__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, Reshape, Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "\n",
        "# Define Capsule Layer\n",
        "class CapsuleLayer(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        self.W = self.add_weight(shape=[input_dim_capsule, self.num_capsule * self.dim_capsule],\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 name='W')\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        u_hat_vecs = tf.reduce_sum(tf.expand_dims(u_vecs, -1) * tf.expand_dims(self.W, 0), axis=-2)\n",
        "        b = tf.zeros(shape=[tf.shape(u_hat_vecs)[0], self.num_capsule])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            s_j = tf.reduce_sum(tf.multiply(c[:, :, None], u_hat_vecs), axis=1)\n",
        "            v_j = self.squash(s_j)\n",
        "            if i < self.routings - 1:\n",
        "                b += tf.reduce_sum(tf.multiply(v_j[:, None, :], u_hat_vecs), axis=-1)\n",
        "        return v_j\n",
        "\n",
        "    def squash(self, s_j):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(s_j), axis=-1, keepdims=True)\n",
        "        scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale * s_j\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Modify the data shapes based on the model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.expand_dims(X_train_resized, axis=-1)  # Add channel dimension\n",
        "X_train_resized = np.repeat(X_train_resized, 3, axis=-1)  # Repeat the channel dimension to match the expected input shape of NASNetMobile\n",
        "\n",
        "# Example: Assuming X_test is another dataset\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.expand_dims(X_test_resized, axis=-1)\n",
        "X_test_resized = np.repeat(X_test_resized, 3, axis=-1)\n",
        "\n",
        "# Define your custom CNN architecture with Capsule Network\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Reshape((-1, 128))(x)  # Reshape to 3D tensor for Capsule Network\n",
        "capsule = CapsuleLayer(num_capsule=10, dim_capsule=16, routings=3)(x)  # Example: 10 capsules with dimension 16\n",
        "capsule = Flatten()(capsule)  # Flatten the output\n",
        "output = Dense(1, activation='sigmoid')(capsule)  # Use a single output unit for binary classification\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=input_tensor, outputs=output)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score, Cohen's Kappa, Matthews Correlation Coefficient, and Balanced Accuracy\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Cohen's Kappa Coefficient:\", kappa)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)\n",
        "\n",
        "# Load NASNetMobile as base model\n",
        "base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define custom CNN architecture with attention mechanism on top of NASNetMobile\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "attention_probs = Dense(np.prod(x.shape[1:]), activation='softmax', name='attention_probs')(x)\n",
        "attention_mul = Multiply()([x, attention_probs])\n",
        "output_layer = Dense(1, activation='sigmoid')(attention_mul)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping and 50 epochs\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "# Print additional metrics\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Calculate Balanced Accuracy\n",
        "balanced_acc = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "8B_ScD_eAZG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Layers using LeakyReLU"
      ],
      "metadata": {
        "id": "uydhDLaTCvZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "0v4c63TfDG-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Global Average Pooling\n",
        "pooled_output = GlobalAveragePooling2D()(nasnet_output)\n",
        "\n",
        "# Dense Layers with LeakyReLU activation\n",
        "x = Dense(512)(pooled_output)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "x = Dense(256)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "rgGwVMKyCxnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "dCwNdTi6DI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Global Average Pooling\n",
        "pooled_output = GlobalAveragePooling2D()(nasnet_output)\n",
        "\n",
        "# Dense Layers with LeakyReLU activation\n",
        "x = Dense(512)(pooled_output)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "x = Dense(256)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "AwUEJe6kDJ7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "M7_dqXQqDLZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Global Average Pooling\n",
        "pooled_output = GlobalAveragePooling2D()(nasnet_output)\n",
        "\n",
        "# Dense Layers with LeakyReLU activation\n",
        "x = Dense(512)(pooled_output)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "x = Dense(256)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation\n",
        "x = Dropout(0.5)(x)  # Dropout layer\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "fp5zVPz-DMmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "Gr5r9oSGDOpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "x4V1OJYsDTfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, MultiHeadAttention, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Reshape the output for Multi-Head Attention\n",
        "nasnet_output_flattened = GlobalAveragePooling2D()(nasnet_output)\n",
        "nasnet_output_reshaped = Reshape((1, nasnet_output_flattened.shape[1]))(nasnet_output_flattened)\n",
        "\n",
        "# Apply Multi-Head Attention\n",
        "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(nasnet_output_reshaped, nasnet_output_reshaped)\n",
        "\n",
        "# Reshape back for Dense layers\n",
        "attention_output_flattened = Flatten()(attention_output)\n",
        "\n",
        "# Add Dense Layers\n",
        "x = Dropout(0.5)(attention_output_flattened)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "sEGXaKSdDTDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "-pBrDxWlD-2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, MultiHeadAttention, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Reshape the output for Multi-Head Attention\n",
        "nasnet_output_flattened = GlobalAveragePooling2D()(nasnet_output)\n",
        "nasnet_output_reshaped = Reshape((1, nasnet_output_flattened.shape[1]))(nasnet_output_flattened)\n",
        "\n",
        "# Apply Multi-Head Attention\n",
        "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(nasnet_output_reshaped, nasnet_output_reshaped)\n",
        "\n",
        "# Reshape back for Dense layers\n",
        "attention_output_flattened = Flatten()(attention_output)\n",
        "\n",
        "# Add Dense Layers\n",
        "x = Dropout(0.5)(attention_output_flattened)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "-vVuhXNmEAQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "5Jx6IxFrEBzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, GlobalAveragePooling2D, MultiHeadAttention, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Reshape the output for Multi-Head Attention\n",
        "nasnet_output_flattened = GlobalAveragePooling2D()(nasnet_output)\n",
        "nasnet_output_reshaped = Reshape((1, nasnet_output_flattened.shape[1]))(nasnet_output_flattened)\n",
        "\n",
        "# Apply Multi-Head Attention\n",
        "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(nasnet_output_reshaped, nasnet_output_reshaped)\n",
        "\n",
        "# Reshape back for Dense layers\n",
        "attention_output_flattened = Flatten()(attention_output)\n",
        "\n",
        "# Add Dense Layers\n",
        "x = Dropout(0.5)(attention_output_flattened)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "i-snkvjSEDHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spatial Dropout"
      ],
      "metadata": {
        "id": "1Zhkx9zCEFca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "DqQ8oQz8EIdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, SpatialDropout2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                            matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Apply Spatial Dropout\n",
        "x = SpatialDropout2D(0.5)(nasnet_output)\n",
        "\n",
        "# Global Average Pooling and Dense Layers\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "EIB1VLcoEHbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "ADKkOgu9Ep6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, SpatialDropout2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                            matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Apply Spatial Dropout\n",
        "x = SpatialDropout2D(0.5)(nasnet_output)\n",
        "\n",
        "# Global Average Pooling and Dense Layers\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "4TRZ7CKpErn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "UzxLKnSiEr-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, SpatialDropout2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                            matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input for the model\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "nasnet_output = nasnet_base(inputs)\n",
        "\n",
        "# Apply Spatial Dropout\n",
        "x = SpatialDropout2D(0.5)(nasnet_output)\n",
        "\n",
        "# Global Average Pooling and Dense Layers\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "Ms7TBl5iEuGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 Regularization"
      ],
      "metadata": {
        "id": "YVwbWIKGEmvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "wcF5dVWdEza4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base with L2 regularization\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "PqGZjrrTEnh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "Y_7SBuyDFHHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base with L2 regularization\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "y7Kv6fyQFI0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "yiV86WYxFJA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base with L2 regularization\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),  # L2 regularization\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "Q-ndAxDyFKh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet Blocks"
      ],
      "metadata": {
        "id": "SOOsHJRhFFaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "66iCiqUTFZG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Changed to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),  # You can adjust the units as needed\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "EdIQxJxFFGGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "MQnDJJpzFbKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Changed to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),  # You can adjust the units as needed\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "--1IC8_-Fa26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "vQRG9PlPFbUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Changed to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),  # You can adjust the units as needed\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train_resized, y_train, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "hv8WsNNaFa7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MixUP Augmentation"
      ],
      "metadata": {
        "id": "rAcR0YbTFuFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 epochs"
      ],
      "metadata": {
        "id": "01Os476pF13T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Change to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Function for Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha <= 0:\n",
        "        return x, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.shape[0]\n",
        "    index = np.random.permutation(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    mixed_y = lam * y + (1 - lam) * y[index]\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Custom generator for Mixup\n",
        "class MixupDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x, y, batch_size=32, alpha=0.2):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.indexes = np.arange(len(self.x))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[batch_indexes]\n",
        "        batch_y = self.y[batch_indexes]\n",
        "\n",
        "        # Apply Mixup augmentation\n",
        "        mixed_x, mixed_y = mixup_data(batch_x, batch_y, alpha=self.alpha)\n",
        "\n",
        "        return mixed_x, mixed_y\n",
        "\n",
        "# Instantiate the data generator\n",
        "train_generator = MixupDataGenerator(X_train_resized, y_train, batch_size=32, alpha=0.2)\n",
        "\n",
        "# Train the model with early stopping and Mixup data generator\n",
        "history = model.fit(train_generator, epochs=50, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "IWHJSpTFF8ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 150 epochs"
      ],
      "metadata": {
        "id": "ivypiidFF3jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Change to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Function for Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha <= 0:\n",
        "        return x, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.shape[0]\n",
        "    index = np.random.permutation(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    mixed_y = lam * y + (1 - lam) * y[index]\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Custom generator for Mixup\n",
        "class MixupDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x, y, batch_size=32, alpha=0.2):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.indexes = np.arange(len(self.x))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[batch_indexes]\n",
        "        batch_y = self.y[batch_indexes]\n",
        "\n",
        "        # Apply Mixup augmentation\n",
        "        mixed_x, mixed_y = mixup_data(batch_x, batch_y, alpha=self.alpha)\n",
        "\n",
        "        return mixed_x, mixed_y\n",
        "\n",
        "# Instantiate the data generator\n",
        "train_generator = MixupDataGenerator(X_train_resized, y_train, batch_size=32, alpha=0.2)\n",
        "\n",
        "# Train the model with early stopping and Mixup data generator\n",
        "history = model.fit(train_generator, epochs=150, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "avAQFyVNFu01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 200 epochs"
      ],
      "metadata": {
        "id": "J7aQ6Pz1F3mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import NASNetMobile  # Change to NASNetMobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve,\n",
        "                             roc_auc_score, precision_recall_curve, auc,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             balanced_accuracy_score, cohen_kappa_score,\n",
        "                             matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Assume X_train and y_train are the original splits\n",
        "\n",
        "# Modify the data shapes based on the NASNetMobile model requirements\n",
        "X_train_resized = np.array([cv2.resize(img, (224, 224)) for img in X_train])\n",
        "X_train_resized = np.repeat(X_train_resized[..., np.newaxis], 3, -1)  # Repeat the channel dimension\n",
        "\n",
        "X_test_resized = np.array([cv2.resize(img, (224, 224)) for img in X_test])\n",
        "X_test_resized = np.repeat(X_test_resized[..., np.newaxis], 3, -1)\n",
        "\n",
        "# Load the pre-trained NASNetMobile model without the top layers\n",
        "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base NASNetMobile model\n",
        "for layer in nasnet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the NASNetMobile base\n",
        "model = Sequential([\n",
        "    nasnet_base,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Add dropout layer to reduce overfitting\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and additional metrics\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(),\n",
        "                                                                      tf.keras.metrics.Recall(),\n",
        "                                                                      tf.keras.metrics.AUC(),\n",
        "                                                                      tf.keras.metrics.AUC(curve='PR')])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Function for Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha <= 0:\n",
        "        return x, y\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.shape[0]\n",
        "    index = np.random.permutation(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    mixed_y = lam * y + (1 - lam) * y[index]\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "# Custom generator for Mixup\n",
        "class MixupDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x, y, batch_size=32, alpha=0.2):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.indexes = np.arange(len(self.x))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[batch_indexes]\n",
        "        batch_y = self.y[batch_indexes]\n",
        "\n",
        "        # Apply Mixup augmentation\n",
        "        mixed_x, mixed_y = mixup_data(batch_x, batch_y, alpha=self.alpha)\n",
        "\n",
        "        return mixed_x, mixed_y\n",
        "\n",
        "# Instantiate the data generator\n",
        "train_generator = MixupDataGenerator(X_train_resized, y_train, batch_size=32, alpha=0.2)\n",
        "\n",
        "# Train the model with early stopping and Mixup data generator\n",
        "history = model.fit(train_generator, epochs=200, validation_data=(X_test_resized, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy, precision, recall, auc_roc, auc_pr = model.evaluate(X_test_resized, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "print(f'AUC-PR: {auc_pr:.2f}')\n",
        "\n",
        "# Confusion matrix\n",
        "test_predictions = (model.predict(X_test_resized) > 0.5).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# ROC curve and AUC score\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "roc_auc = roc_auc_score(y_test, test_predictions)\n",
        "print(f'AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Precision for class 1 (Positive)\n",
        "precision = precision_score(y_test, test_predictions)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Recall for class 1 (Positive)\n",
        "recall = recall_score(y_test, test_predictions)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, test_predictions)\n",
        "print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "# Precision-Recall curve and PR AUC\n",
        "precision, recall, _ = precision_recall_curve(y_test, test_predictions)\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n",
        "\n",
        "# Cohen's Kappa Coefficient\n",
        "kappa = cohen_kappa_score(y_test, test_predictions)\n",
        "print(f\"Cohen's Kappa Coefficient: {kappa:.2f}\")\n",
        "\n",
        "# Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, test_predictions)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, test_predictions)\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the time taken\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Count the number of Monkeypox and Non-Monkeypox predictions\n",
        "total_samples = len(test_predictions)\n",
        "monkeypox_predictions = np.sum(test_predictions == 1)\n",
        "non_monkeypox_predictions = np.sum(test_predictions == 0)\n",
        "\n",
        "# Calculate the percentage for Monkeypox and Non-Monkeypox\n",
        "monkeypox_percentage = (monkeypox_predictions / total_samples) * 100\n",
        "non_monkeypox_percentage = (non_monkeypox_predictions / total_samples) * 100\n",
        "\n",
        "# Print the percentages\n",
        "print(f\"Monkeypox Predictions: {monkeypox_percentage:.2f}% ({monkeypox_predictions}/{total_samples})\")\n",
        "print(f\"Non-Monkeypox Predictions: {non_monkeypox_percentage:.2f}% ({non_monkeypox_predictions}/{total_samples})\")\n",
        "\n",
        "# Custom statement for dataset summary\n",
        "if monkeypox_percentage > non_monkeypox_percentage:\n",
        "    print(f\"The model predicted more Monkeypox cases than non-Monkeypox cases.\")\n",
        "else:\n",
        "    print(f\"The model predicted more non-Monkeypox cases than Monkeypox cases.\")\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "eywptSKNF49w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}